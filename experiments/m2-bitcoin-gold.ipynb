{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ebaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Money Supply\n",
    "leading_indicators = 'M2SL'\n",
    "equities = ['BTC-USD','GLD','SLV']\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dash import Dash, html, dcc, dash_table\n",
    "import plotly.express as px\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate RMSE, MAE, and RÂ².\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "def data_stock_data(ticker):\n",
    "    \"\"\"\n",
    "    Fetch monthly adjusted stock data from yfinance.\n",
    "    \"\"\"\n",
    "    output_dir = Path(\"C:/Users/Steel/Desktop/Projects/intel-sweep/intel-sweep/src/data/yf_monthly\")\n",
    "    logger.info(f\"Fetching monthly data for {ticker}\")\n",
    "    # Use yfinance download with monthly interval\n",
    "    try:\n",
    "        #Maybe we already have the data saved locally\n",
    "        data = pd.read_csv(f\"C:/Users/Steel/Desktop/Projects/intel-sweep/intel-sweep/src/data/yf_monthly/{ticker}_monthly.csv\")\n",
    "    except FileNotFoundError:\n",
    "        #If not, download it\n",
    "        data = yf.download(ticker, period=\"max\", interval=\"1mo\", auto_adjust=True, progress=False)\n",
    "        if data.empty:\n",
    "            raise ValueError(f\"No data returned for {ticker}\")\n",
    "\n",
    "        # Reset and rename columns for consistency with Alpha Vantage\n",
    "        data.reset_index(inplace=True)\n",
    "        data.rename(columns={\n",
    "            'Date': 'Date',\n",
    "            'Open': 'Open',\n",
    "            'High': 'High',\n",
    "            'Low': 'Low',\n",
    "            'Close': 'Close',\n",
    "            'Volume': 'Volume'\n",
    "        }, inplace=True)\n",
    "        # Add 'Adj Close' (already adjusted due to auto_adjust=True)\n",
    "        data.columns = data.columns.get_level_values(0)\n",
    "        data['Adj Close'] = data['Close']\n",
    "\n",
    "        # Keep relevant columns\n",
    "        data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]\n",
    "        data['Date'] = pd.to_datetime(data['Date']).dt.strftime('%Y-%m-%d')\n",
    "        data = data.sort_values('Date')\n",
    "        data = data.round(4)\n",
    "        output_file = output_dir / f\"{ticker.upper()}_monthly.csv\"\n",
    "        data.to_csv(output_file, index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_indicator_data(series_id, start_date=\"2000-01-01\", end_date=\"2025-04-30\",API_KEY= \"17188a6953269ab608ba14c3e3d8fb02\" ):\n",
    "    print(\"Test\")\n",
    "    try:\n",
    "        #Try to read the CSV file first\n",
    "        df = pd.read_csv(f\"C:/Users/Steel/Desktop/Projects/intel-sweep/intel-sweep/src/data/fred_economic_indicators/{series_id}.csv\", index_col=\"date\", parse_dates=True)\n",
    "        \n",
    "        print(\"Ttest3\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Test2\")\n",
    "        base_url = \"https://api.stlouisfed.org/fred/series/observations\"\n",
    "        long_series = []\n",
    "        # If the CSV file does not exist or cannot be read, fetch from FRED API\n",
    "        alt_start_date = \"2017-06-01\" if series_id in long_series else start_date\n",
    "        params = {\n",
    "            \"series_id\": series_id,\n",
    "            \"api_key\": API_KEY,\n",
    "            \"realtime_start\": alt_start_date,\n",
    "            \"realtime_end\": end_date,\n",
    "            \"observation_start\": alt_start_date,\n",
    "            \"observation_end\": end_date,\n",
    "            # \"frequency\": frequency,\n",
    "            \"file_type\": \"json\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params)\n",
    "            print(response.content)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if \"observations\" not in data or not data[\"observations\"]:\n",
    "                logger.warning(f\"No observations for {series_id}\")\n",
    "                return None\n",
    "                \n",
    "            df = pd.DataFrame(data[\"observations\"])[[\"date\", \"value\"]]\n",
    "            df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "            df = df.rename(columns={\"value\": series_id}).set_index(\"date\")\n",
    "            df.to_csv(f\"C:/Users/Steel/Desktop/Projects/intel-sweep/intel-sweep/src/data/fred_economic_indicators/{series_id}.csv\", index=True)\n",
    "            return df\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "            logger.error(f\"Error fetching {series_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "def merge_indicators_with_stock_data(stock_list, indicators, start_date=\"2000-01-01\", end_date=\"2025-04-30\"):\n",
    "    \"\"\"Merge stock data with economic indicators.\"\"\"\n",
    "    logger.info(\"Merging stock data with economic indicators\")\n",
    "    merged_data =  get_stock_data(stock_list[0], start_date, end_date)\n",
    "    for stock in stock_list[1:]:\n",
    "        stock_data = get_stock_data(stock, start_date, end_date)\n",
    "        if stock_data is not None:\n",
    "            merged_data = merged_data.join(indicator_data, how='left')\n",
    "\n",
    "\n",
    "    for indicator in indicators:\n",
    "        indicator_data = get_indicator_data(indicator, start_date, end_date)\n",
    "        if indicator_data is not None:\n",
    "            merged_data = merged_data.join(indicator_data, how='left')\n",
    "            logger.info(f\"Added indicator {indicator} to stock data\")\n",
    "        else:\n",
    "            logger.warning(f\"Indicator {indicator} data not available\")\n",
    "\n",
    "    # Fill NaN values with forward fill and backward fill\n",
    "    merged_data.fillna(method='ffill', inplace=True)\n",
    "    merged_data.fillna(method='bfill', inplace=True)\n",
    "    merged_data.to_csv(f\"C:/Users/Steel/Desktop/Projects/intel-sweep/intel-sweep/data/combined/m2-bitcoin-gold_combined.csv\")\n",
    "    return merged_data\n",
    "\n",
    "def engineer_features(df: pd.DataFrame, target_col: str, lag_periods: List[int] = [1, 3, 6], ma_windows: List[int] = [3, 12]) -> pd.DataFrame:\n",
    "    \"\"\"Add lagged variables and moving averages for features.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Engineering features: lags and moving averages\")\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Create lagged features for all columns\n",
    "        for col in df.columns:\n",
    "            for lag in lag_periods:\n",
    "                df[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "        \n",
    "        # Create moving averages for target column (stock price)\n",
    "        for window in ma_windows:\n",
    "            df[f'{target_col}_ma{window}'] = df[target_col].rolling(window=window).mean()\n",
    "        \n",
    "        # Drop rows with NaN values from lags or moving averages\n",
    "        df = df.fillna(method='bfill').fillna(method='ffill')\n",
    "        logger.info(f\"Feature engineering complete, resulting shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in feature engineering: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "class StockPriceNN(nn.Module):\n",
    "    \"\"\"Feed-forward neural network for stock price forecasting.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=64):\n",
    "        super(StockPriceNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class StockPriceLSTM(nn.Module):\n",
    "    \"\"\"LSTM network for stock price forecasting.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2):\n",
    "        super(StockPriceLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train_pytorch_forecast(data_path, target, selected_features, horizon=1, params=None, use_lstm=False):\n",
    "    \"\"\"Train PyTorch model for h-month-ahead forecasting.\"\"\"\n",
    "    print(\"Test: Entering train_pytorch_forecast\")\n",
    "    if params is None:\n",
    "        params = {'hidden_size': 64, 'epochs': 100, 'learning_rate': 0.001, 'batch_size': 16}\n",
    "\n",
    "    try:\n",
    "        df = pd.read_parquet(data_path)\n",
    "        logger.info(f\"Loaded data for {target} with shape {df.shape}\")\n",
    "        if target not in df.columns:\n",
    "            logger.error(f\"Target column {target} not found in data.\")\n",
    "            return None, None, None\n",
    "\n",
    "        df[target] = pd.to_numeric(df[target], errors='coerce')\n",
    "        if df[target].isna().any():\n",
    "            logger.warning(f\"Target {target} contains {df[target].isna().sum()} NaN values after conversion.\")\n",
    "            df = df.dropna(subset=[target])\n",
    "            if df.empty:\n",
    "                logger.error(f\"No valid data for {target} after numeric conversion.\")\n",
    "                return None, None, None\n",
    "        logger.info(f\"Target {target} dtype: {df[target].dtype}\")\n",
    "\n",
    "        if len(df) < horizon:\n",
    "            logger.error(f\"Dataset too short ({len(df)} rows) for horizon {horizon}.\")\n",
    "            return None, None, None\n",
    "\n",
    "        df[target + f'_t{horizon}'] = df[target].shift(-horizon)\n",
    "        df = df.dropna(subset=selected_features + [target + f'_t{horizon}'])\n",
    "        logger.info(f\"Data shape after NaN handling: {df.shape}\")\n",
    "        if df.empty:\n",
    "            logger.error(f\"No valid data for {target} after NaN handling.\")\n",
    "            return None, None, None\n",
    "\n",
    "        X = df[selected_features].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "        y = df[target + f'_t{horizon}']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "        logger.info(f\"Train indices: {X_train.index[:5].tolist()}, Test indices: {X_test.index[:5].tolist()}\")\n",
    "        y_train = pd.to_numeric(y_train, errors='coerce')\n",
    "        y_test = pd.to_numeric(y_test, errors='coerce')\n",
    "        non_nan_train = ~y_train.isna()\n",
    "        non_nan_test = ~y_test.isna()\n",
    "        X_train = X_train[non_nan_train]\n",
    "        y_train = y_train[non_nan_train]\n",
    "        X_test = X_test[non_nan_test]\n",
    "        y_test = y_test[non_nan_test]\n",
    "        logger.info(f\"Train size after NaN filter: {len(X_train)}, Test size after NaN filter: {len(X_test)}\")\n",
    "        if len(X_train) == 0 or len(X_test) == 0:\n",
    "            logger.error(f\"No valid train/test data for {target} after NaN handling.\")\n",
    "            return None, None, None\n",
    "\n",
    "        print(\"Test2: Before scaling\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        if use_lstm:\n",
    "            # Reshape for LSTM [samples, timesteps, features]\n",
    "            X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
    "            X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
    "\n",
    "        X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "        y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1)\n",
    "        X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "        y_test_tensor = torch.FloatTensor(y_test.values).reshape(-1, 1)\n",
    "\n",
    "        input_size = X.shape[1]\n",
    "        model = StockPriceLSTM(input_size, params['hidden_size']) if use_lstm else StockPriceNN(input_size, params['hidden_size'])\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "        mlflow.set_experiment(\"stock_price_forecasting\")\n",
    "        with mlflow.start_run(run_name=f\"{target}_horizon{horizon}\"):\n",
    "            mlflow.set_tag(\"ticker\", target.split('_')[0])\n",
    "            mlflow.set_tag(\"horizon\", horizon)\n",
    "            mlflow.set_tag(\"model_type\", \"LSTM\" if use_lstm else \"NN\")\n",
    "            model.train()\n",
    "            for epoch in range(params['epochs']):\n",
    "                for i in range(0, len(X_train_tensor), params['batch_size']):\n",
    "                    batch_X = X_train_tensor[i:i + params['batch_size']]\n",
    "                    batch_y = y_train_tensor[i:i + params['batch_size']]\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    logger.info(f\"Epoch {epoch+1}/{params['epochs']} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(X_test_tensor).numpy().flatten()\n",
    "            # Bias correction\n",
    "            with torch.no_grad():\n",
    "                y_train_pred = model(X_train_tensor).numpy().flatten()\n",
    "            bias = np.mean(y_train_pred - y_train)\n",
    "            y_pred += bias\n",
    "            metrics = calculate_metrics(y_test, y_pred)\n",
    "            logger.info(f\"Metrics for {target} (horizon {horizon}): {metrics}\")\n",
    "\n",
    "            current_prices = df.loc[y_test.index, target]\n",
    "            logger.info(f\"Current prices shape: {current_prices.shape}, y_test shape: {y_test.shape}\")\n",
    "            price_change = (y_pred - current_prices) / current_prices * 100\n",
    "            signals = np.where(price_change > 5, \"Buy\", np.where(price_change < -5, \"Sell\", \"Hold\"))\n",
    "            backtest_df = pd.DataFrame({\n",
    "                \"Date\": y_test.index,\n",
    "                \"Actual\": y_test,\n",
    "                \"Predicted\": y_pred,\n",
    "                \"Current_Price\": current_prices,\n",
    "                \"Signal\": signals\n",
    "            })\n",
    "            backtest_df.to_csv(f\"backtest_{target}_horizon{horizon}.csv\")\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_metrics(metrics)\n",
    "            mlflow.log_param(\"features\", \", \".join(selected_features))\n",
    "            mlflow.log_artifact(f\"backtest_{target}_horizon{horizon}.csv\")\n",
    "            mlflow.pytorch.log_model(model, \"pytorch_model\", input_example=X_train_scaled[:1])\n",
    "            model_uri = f\"runs:/{mlflow.active_run().info.run_id}/pytorch_model\"\n",
    "            registered_model = mlflow.register_model(model_uri, f\"StockPricePyTorch_Horizon{horizon}\")\n",
    "            logger.info(f\"Model registered: {registered_model.name} version {registered_model.version}\")\n",
    "\n",
    "        return model, metrics, backtest_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_pytorch_forecast for {target} horizon {horizon}: {str(e)}\")\n",
    "        logger.error(f\"Error in train_pytorch_forecast for {target} horizon {horizon}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_dashboard(results):\n",
    "    \"\"\"Create a Dash dashboard for buy/sell signals.\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Test: Main block\")\n",
    "    tickers = pd.read_csv(\"C:/Users/Steel/Desktop/Projects/intel-sweep/intel-sweep/src/data/equities.csv\")['Symbol'].tolist()\n",
    "    logger.info(f\"Processing {len(tickers)} tickers: {tickers}\")\n",
    "    # Process a single ticker for testing\n",
    "    results = process_stock(\"AAPL\")\n",
    "    print(f\"Results: {results}\")\n",
    "    \n",
    "    if results:\n",
    "        # Create dashboard for AAPL\n",
    "        create_dashboard(results)\n",
    "        \n",
    "        # Process all tickers\n",
    "        results = Parallel(n_jobs=-1)(delayed(process_stock)(ticker) for ticker in tickers)  # Limit for demo\n",
    "        results = [r for r in sum([r if r else [] for r in results], []) if r is not None]\n",
    "        results_df = pd.DataFrame(results)\n",
    "        if not results_df.empty:\n",
    "            logger.info(f\"Results DataFrame shape: {results_df.shape}\")\n",
    "            top_10 = results_df.sort_values(by='r2', ascending=False).head(10)\n",
    "            logger.info(f\"Top 10 models by RÂ²:\\n{top_10[['ticker', 'horizon', 'r2', 'rmse', 'mae']]}\")\n",
    "            top_10.to_csv(\"top_10_forecasts_by_r2.csv\", index=False)\n",
    "\n",
    "            with mlflow.start_run(run_name=\"Top_10_Forecasts_Summary\"):\n",
    "                mlflow.log_artifact(\"top_10_forecasts_by_r2.csv\")\n",
    "                for i, row in top_10.iterrows():\n",
    "                    mlflow.log_metric(f\"{row['ticker']}_horizon{row['horizon']}_r2\", row['r2'])\n",
    "                    mlflow.log_metric(f\"{row['ticker']}_horizon{row['horizon']}_rmse\", row['rmse'])\n",
    "                    mlflow.log_metric(f\"{row['ticker']}_horizon{row['horizon']}_mae\", row['mae'])\n",
    "\n",
    "            create_dashboard(results)  # Dashboard for all tickers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
